{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "##### *Mintae Kim, Hybrid Robotics Lab, UC Berkeley*\n",
    "##### 07/16/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of SARSA\n",
    "Since SARSA is a q function-based algorithm, we only need an agent class to implement it. Agent would have a policy within itself.\n",
    "\n",
    "```python\n",
    "class SARSAgent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        pass\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.step_size = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        # Initialize q function table with zeros\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # Update q function with <s, a, r, s', a'>\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        state, next_state = str(state), str(next_state)\n",
    "        current_q = self.q_table[state][action]\n",
    "        next_state_q = self.q_table[next_state][next_action]\n",
    "        td = reward + self.discount_factor * next_state_q - current_q\n",
    "        new_q = current_q + self.step_size * td\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    # Return action with $\\epsilon$-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Greedy action\n",
    "            state = str(state)\n",
    "            q_list = self.q_table[state]\n",
    "            action = arg_max(q_list)\n",
    "        return action\n",
    "\n",
    "\n",
    "# Return best action based on the value of q function\n",
    "def arg_max(q_list):\n",
    "    max_idx_list = np.argwhere(q_list == np.amax(q_list))\n",
    "    max_idx_list = max_idx_list.flatten().tolist()\n",
    "    return random.choice(max_idx_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = SARSAgent(actions=list(range(env.n_actions)))\n",
    "\n",
    "    for episode in range(1000):\n",
    "        # Initialize game environment and state\n",
    "        state = env.reset()\n",
    "        # Choose action based on current state\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        while True:\n",
    "            env.render()\n",
    "\n",
    "            # After taking the action, get next state, reward, and whether the episode is done or not\n",
    "            next_state, reward, done = env.step(action)\n",
    "            # Choose next action at next state\n",
    "            next_action = agent.get_action(next_state)\n",
    "            # Update q function with <s,a,r,s',a'>\n",
    "            agent.learn(state, action, reward, next_state, next_action)\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            # Displacy every q function on the screen\n",
    "            env.print_value_all(agent.q_table)\n",
    "\n",
    "            if done:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
